{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## はじめに\n",
    "\n",
    "本ハンズオンでは機械学習の基礎を体験頂きます。コードの詳細は気にせずに、どういった手順で何をしようとしているのか、機械学習モデル構築の流れをご確認下さい。また不明な単語などありましたら、参考文献などでご確認を頂けたらと思います。\n",
    "\n",
    "今回は機械学習モデルのチュートリアルでは定番のTitanicのデータを活用します。\n",
    "\n",
    "今後のプロトタイピングを実施するに辺り、本ハンズオンを通してご確認頂きたい概念は下記です。\n",
    "- 教師あり学習とは\n",
    "- 探索的データ解析(EDA)から特徴量を作成する流れ\n",
    "- 欠損データへの対応\n",
    "- カテゴリカル変数の取り扱い(ダミー変数化など)\n",
    "- 機械学習アルゴリズムのハイパーパラメータとは\n",
    "- 交差検証とは\n",
    "\n",
    "逆に、現時点で時間をかけて頂かなくて良い点は下記になります。\n",
    "- 機械学習アルゴリズムそのもの(今回で言えば決定木アルゴリズム）\n",
    "- Pythonライブラリのそれぞれの詳細(numpy, pandas, scikit-learnなど)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-08-16 00:12:00--  http://bit.ly/2KEZlRT\n",
      "Resolving bit.ly (bit.ly)... 67.199.248.10, 67.199.248.11\n",
      "Connecting to bit.ly (bit.ly)|67.199.248.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://handson-taitanic.s3.amazonaws.com/titanic-data.zip [following]\n",
      "--2019-08-16 00:12:00--  https://handson-taitanic.s3.amazonaws.com/titanic-data.zip\n",
      "Resolving handson-taitanic.s3.amazonaws.com (handson-taitanic.s3.amazonaws.com)... 54.231.72.251\n",
      "Connecting to handson-taitanic.s3.amazonaws.com (handson-taitanic.s3.amazonaws.com)|54.231.72.251|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 35270 (34K) [application/zip]\n",
      "Saving to: ‘2KEZlRT’\n",
      "\n",
      "2KEZlRT             100%[===================>]  34.44K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2019-08-16 00:12:00 (15.6 MB/s) - ‘2KEZlRT’ saved [35270/35270]\n",
      "\n",
      "Archive:  2KEZlRT\n",
      "   creating: titanic-data/\n",
      "  inflating: titanic-data/.DS_Store  \n",
      "   creating: __MACOSX/\n",
      "   creating: __MACOSX/titanic-data/\n",
      "  inflating: __MACOSX/titanic-data/._.DS_Store  \n",
      "  inflating: titanic-data/test.csv   \n",
      "  inflating: __MACOSX/titanic-data/._test.csv  \n",
      "  inflating: titanic-data/train.csv  \n",
      "  inflating: __MACOSX/titanic-data/._train.csv  \n",
      "  inflating: __MACOSX/._titanic-data  \n"
     ]
    }
   ],
   "source": [
    "!wget http://bit.ly/2KEZlRT\n",
    "!unzip -o 2KEZlRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの確認\n",
    "まずは元データについて確認をします。今回は簡単にデータサイズ、カラム名、サンプルの抽出を実施します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====元データのサイズは下記====\n",
      "(891, 12)\n",
      "(418, 11)\n",
      "=====元データのカラムは下記====\n",
      "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
      "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>622</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Kimball, Mr. Edwin Nelson Jr</td>\n",
       "      <td>male</td>\n",
       "      <td>42.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11753</td>\n",
       "      <td>52.5542</td>\n",
       "      <td>D19</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Harrison, Mr. William</td>\n",
       "      <td>male</td>\n",
       "      <td>40.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112059</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>B94</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>470</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Baclini, Miss. Helene Barbara</td>\n",
       "      <td>female</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2666</td>\n",
       "      <td>19.2583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>853</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Boulos, Miss. Nourelain</td>\n",
       "      <td>female</td>\n",
       "      <td>9.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2678</td>\n",
       "      <td>15.2458</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>739</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Ivanoff, Mr. Kanio</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349201</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass                           Name     Sex  \\\n",
       "621          622         1       1   Kimball, Mr. Edwin Nelson Jr    male   \n",
       "263          264         0       1          Harrison, Mr. William    male   \n",
       "469          470         1       3  Baclini, Miss. Helene Barbara  female   \n",
       "852          853         0       3        Boulos, Miss. Nourelain  female   \n",
       "738          739         0       3             Ivanoff, Mr. Kanio    male   \n",
       "\n",
       "       Age  SibSp  Parch  Ticket     Fare Cabin Embarked  \n",
       "621  42.00      1      0   11753  52.5542   D19        S  \n",
       "263  40.00      0      0  112059   0.0000   B94        S  \n",
       "469   0.75      2      1    2666  19.2583   NaN        C  \n",
       "852   9.00      1      1    2678  15.2458   NaN        C  \n",
       "738    NaN      0      0  349201   7.8958   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ライブラリーのインポート\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#データの読み込み\n",
    "train = pd.read_csv(\"titanic-data/train.csv\")\n",
    "test = pd.read_csv(\"titanic-data/test.csv\")\n",
    "\n",
    "#データセットのサイズの確認\n",
    "print(\"=====元データのサイズは下記====\")\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "\n",
    "#データセットのカラム名の確認\n",
    "print(\"=====元データのカラムは下記====\")\n",
    "print(train.columns)\n",
    "\n",
    "#どのようなデータセットになっているのか、サンプルを5個抽出\n",
    "train.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回のデータセットで変数の種類はそれぞれ下記となっています。\n",
    "- Boolian: Survived\n",
    "- Numerical: Age (連続値), Fare (連続値), SibSp (離散値), Parch (離散値)\n",
    "- Categorical: Survived, Sex, Embarked, Pclass\n",
    "- Alphanumeric: Ticket, Cabin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 欠損値の確認\n",
    "今回のデータはいくつかの特徴量において欠損があるためそれぞれに対応する必要があります。先程のサンプルでも`Cabin`の各データは全て\"NaN\"で欠損しています。他の特徴にも欠損が無いか見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#欠損値の確認\n",
    "print(pd.isnull(train).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA(Exploratory Data Analysis)：探索的データ解析\n",
    "EDAの結果、下記のようなことがわかります。\n",
    "\n",
    "- 今回のデータは学習データに891人、テストデータに418人の乗客がそれぞれいる\n",
    "- Sex: 女性の方が生存しやすい\n",
    "- SibSp/Parch: 一人で旅行している乗客ほど生存しやすい\n",
    "- Age: 幼い子どもほど生存しやすい\n",
    "- Pclass: Pclassの高い乗客ほど生存しやすい\n",
    "- 約2割のデータで年齢が欠損しています。年齢によって生存率が大きく異なる\n",
    "- キャビンについては約8割のデータが欠損している\n",
    "\n",
    "その他にも今回のデータセットを確認すると様々なことがわかります。本ハンズオンでは詳細を確認しませんので、ご興味ある方が、他の方々が実施されている内容をご参考に下さい。\n",
    "- [Scikit-Learn ML from Start to Finish](https://www.kaggle.com/jeffd23/scikit-learn-ml-from-start-to-finish)\n",
    "- [Titanic Survival Predictions (Beginner)](https://www.kaggle.com/nadintamer/titanic-survival-predictions-beginner)\n",
    "- [Titanic top 10% - My first competition](https://www.kaggle.com/benteo/titanic-top-10-my-first-competition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ前処理と特徴量作成\n",
    "ここから先は現在あるデータを用いて機械学習モデルの精度が向上するようなデータを作成します。\n",
    "- 欠損しているデータを活用できる形へ変換したり埋めたりする\n",
    "- 機械学習モデルにとってより意味のある特徴量を作成する\n",
    "- カテゴリカル変数を機械学習モデルへ投入できるようなダミー変数へと変換する\n",
    "\n",
    "下記の方針で前処理と特徴量作成を実施します。\n",
    "- Cabin：記録が残っている乗客の方が生存率が高いため、記録あり、無し(欠損)、を特徴量とする\n",
    "- Embarked：乗客の多くがSouthamptonから乗船しているので、記載が無い乗客もSouthamptonから乗船したことにして欠損埋めをする\n",
    "- Age：年齢が若いほうが生存が高いが、欠損が多いため活用が難しい。他のデータから年齢を推測した上で、グループ化して活用\n",
    "    - `Name`特徴量中から`Title`を抽出\n",
    "    - `Title`から年齢がどの程度が推察し`AgeGroup`特徴量を作成\n",
    "- Fare：欠損値を適当な値で埋めた後に四分位毎にグループ化\n",
    "\n",
    "最終的に機械学習モデルが学習できるようカテゴリカル変数をダミー変数化します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cabin特徴量\n",
    "`Cabin`に記録が残っている乗客の方が生存率が高いため、記録あり、無し(欠損)、を特徴量とする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    0\n",
       "3    1\n",
       "4    0\n",
       "Name: Cabin, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Cabin\"] = train[\"Cabin\"].notnull().astype('int')\n",
    "test[\"Cabin\"] = test[\"Cabin\"].notnull().astype('int')\n",
    "\n",
    "#作った特徴量の確認\n",
    "train[\"Cabin\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embarked特徴量\n",
    "乗客の多くがSouthamptonから乗船しているので、記載が無い乗客もSouthamptonから乗船したこととして欠損埋めを実施。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Southampton(S)からの乗客数:644\n",
      "Cherbourg(C)からの乗客数:168\n",
      "Queenstown(Q)からの乗客数:77\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    S\n",
       "1    C\n",
       "2    S\n",
       "3    S\n",
       "4    S\n",
       "Name: Embarked, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Southampton(S)からの乗客数:\" + str(train[train[\"Embarked\"] == \"S\"].shape[0]))\n",
    "print(\"Cherbourg(C)からの乗客数:\" + str(train[train[\"Embarked\"] == \"C\"].shape[0]))\n",
    "print(\"Queenstown(Q)からの乗客数:\"+ str(train[train[\"Embarked\"] == \"Q\"].shape[0]))\n",
    "\n",
    "train = train.fillna({\"Embarked\": \"S\"})\n",
    "test = train.fillna({\"Embarked\": \"S\"})\n",
    "\n",
    "#作った特徴量の確認\n",
    "train[\"Embarked\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age特徴量\n",
    "元のデータは多くの人の年齢情報が欠損しています。今回は名前についている`Title`から年齢を推測して活用しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      Mr\n",
      "1     Mrs\n",
      "2    Miss\n",
      "3     Mrs\n",
      "4      Mr\n",
      "Name: Title, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Master</td>\n",
       "      <td>0.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Miss</td>\n",
       "      <td>0.702703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mr</td>\n",
       "      <td>0.156673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mrs</td>\n",
       "      <td>0.793651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rare</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Royal</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Title  Survived\n",
       "0  Master  0.575000\n",
       "1    Miss  0.702703\n",
       "2      Mr  0.156673\n",
       "3     Mrs  0.793651\n",
       "4    Rare  0.285714\n",
       "5   Royal  1.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#年齢をそのまま使わずにグループに変換\n",
    "train[\"Age\"] = train[\"Age\"].fillna(-0.5)\n",
    "test[\"Age\"] = test[\"Age\"].fillna(-0.5)\n",
    "bins = [-1, 0, 5, 12, 18, 24, 35, 60, np.inf]\n",
    "labels = [\"Unknown\", \"Baby\", \"Child\", \"Teenager\", \"Student\", \"Young Adult\", \"Adult\", \"Senior\"]\n",
    "train[\"AgeGroup\"] = pd.cut(train[\"Age\"], bins, labels = labels)\n",
    "test[\"AgeGroup\"] = pd.cut(test[\"Age\"], bins, labels = labels)\n",
    "\n",
    "#学習データとテストデータの`Name`特徴量から`Title`部分を抽出\n",
    "combine = [train, test]\n",
    "for dataset in combine:\n",
    "    dataset[\"Title\"] = dataset.Name.str.extract(\" ([A-Za-z]+)\\.\", expand=False)\n",
    "    \n",
    "#様々なタイトルをより一般的なタイトルへ変換します\n",
    "for dataset in combine:\n",
    "    dataset[\"Title\"] = dataset[\"Title\"].replace([\"Lady\", \"Capt\", \"Col\",\n",
    "    \"Don\", \"Dr\", \"Major\", \"Rev\", \"Jonkheer\", \"Dona\"], \"Rare\")\n",
    "    \n",
    "    dataset[\"Title\"] = dataset[\"Title\"].replace([\"Countess\", \"Lady\", \"Sir\"], \"Royal\")\n",
    "    dataset[\"Title\"] = dataset[\"Title\"].replace(\"Mlle\", \"Miss\")\n",
    "    dataset[\"Title\"] = dataset[\"Title\"].replace(\"Ms\", \"Miss\")\n",
    "    dataset[\"Title\"] = dataset[\"Title\"].replace(\"Mme\", \"Mrs\")\n",
    "    dataset[\"Title\"] = dataset[\"Title\"].fillna(\"NaN\")\n",
    "\n",
    "#作った特徴量の確認\n",
    "print(train[\"Title\"].head())\n",
    "\n",
    "train[[\"Title\", \"Survived\"]].groupby([\"Title\"], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Mr`がついている方の生存率が極端に低い一方、`Miss`や`Mrs`の生存率が高いことがわかります。Royalは確実に生き残っているところには感じ入るものがあります。指名から抽出したタイトルをもとにして、どの年齢カテゴリなのかを推察します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Student\n",
       "1          Adult\n",
       "2    Young Adult\n",
       "3    Young Adult\n",
       "4    Young Adult\n",
       "Name: AgeGroup, dtype: category\n",
       "Categories (8, object): [Unknown < Baby < Child < Teenager < Student < Young Adult < Adult < Senior]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_title_mapping = {\"Mr\": \"Young Adult\", \"Miss\": \"Student\", \"Mrs\": \"Adult\", \"Master\": \"Baby\", \"Royal\": \"Adult\", \"Rare\": \"Adult\"}\n",
    "\n",
    "for x in range(len(train[\"AgeGroup\"])):\n",
    "    if train[\"AgeGroup\"][x] == \"Unknown\":\n",
    "        train[\"AgeGroup\"][x] = age_title_mapping[train[\"Title\"][x]]\n",
    "        \n",
    "for x in range(len(test[\"AgeGroup\"])):\n",
    "    if test[\"AgeGroup\"][x] == \"Unknown\":\n",
    "        test[\"AgeGroup\"][x] = age_title_mapping[test[\"Title\"][x]]\n",
    "        \n",
    "        \n",
    "#作った特徴量の確認        \n",
    "train[\"AgeGroup\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fare特徴量\n",
    "欠損値を欠損していたことがわかりやすい適当な値で埋めた上で、四分位毎でビンで区切って離散化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1_quartile\n",
       "1    4_quartile\n",
       "2    1_quartile\n",
       "3    4_quartile\n",
       "4    2_quartile\n",
       "Name: Fare, dtype: category\n",
       "Categories (5, object): [Unknown < 1_quartile < 2_quartile < 3_quartile < 4_quartile]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Fare\"] = train[\"Fare\"].fillna(-0.5)\n",
    "test[\"Fare\"] = train[\"Fare\"].fillna(-0.5)\n",
    "bins = (-1, 0, 8, 15, 31, 1000)\n",
    "\n",
    "group_names = [\"Unknown\", \"1_quartile\", \"2_quartile\", \"3_quartile\", \"4_quartile\"]\n",
    "\n",
    "train[\"Fare\"] = pd.cut(train[\"Fare\"], bins, labels=group_names)\n",
    "test[\"Fare\"] = pd.cut(test[\"Fare\"], bins, labels=group_names)\n",
    "\n",
    "#作った特徴量の確認\n",
    "train[\"Fare\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ダミー変数化(ダミーエンコーディング)\n",
    "カテゴリカル変数をダミー変数化(ダミーエンコーディング)します。対象となるのは、`Title`、`Embarked`、`AgeGroup`、`Fare`、`Sex`です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#処理しやすいように学習データとテストデータを結合\n",
    "data = pd.concat([train,test], axis=0, sort=\"True\")\n",
    "\n",
    "title_dummies = pd.get_dummies(data[\"Title\"], prefix=\"Title\")\n",
    "data = pd.concat([data, title_dummies], axis=1)\n",
    "data = data.drop([\"Title\"], axis=1)\n",
    "\n",
    "embarked_dummies = pd.get_dummies(data[\"Embarked\"], prefix=\"Embarked\")\n",
    "data = pd.concat([data, embarked_dummies], axis=1)\n",
    "data = data.drop([\"Embarked\"], axis=1)\n",
    "\n",
    "agegroup_dummies = pd.get_dummies(data[\"AgeGroup\"], prefix=\"AgeGroup\")\n",
    "data = pd.concat([data, agegroup_dummies], axis=1)\n",
    "data = data.drop([\"AgeGroup\"], axis=1)\n",
    "\n",
    "fare_dummies = pd.get_dummies(data[\"Fare\"], prefix=\"Fare\")\n",
    "data = pd.concat([data, fare_dummies], axis=1)\n",
    "data = data.drop([\"Fare\"], axis=1)\n",
    "\n",
    "sex_mapping = {\"male\": 0, \"female\": 1}\n",
    "data[\"Sex\"] = data[\"Sex\"].map(sex_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特徴量の削除\n",
    "あまり有用な情報がなかったり特徴量作成のベースとして使った特徴量を削除します。対象となるのは`Age`、`Name`、`Ticket`、`PassengerId`です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使わない特徴量の削除\n",
    "data = data.drop([\"Age\", \"Ticket\", \"Name\", \"PassengerId\"], axis = 1)\n",
    "\n",
    "#結合したデータを再び学習データとテストデータへ分離\n",
    "train = data.iloc[:len(train)]\n",
    "test = data.iloc[-len(test):].drop([\"Survived\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの確認\n",
    "最終的にモデル構築に使うデータを確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Title_Master</th>\n",
       "      <th>Title_Miss</th>\n",
       "      <th>Title_Mr</th>\n",
       "      <th>Title_Mrs</th>\n",
       "      <th>...</th>\n",
       "      <th>AgeGroup_Teenager</th>\n",
       "      <th>AgeGroup_Student</th>\n",
       "      <th>AgeGroup_Young Adult</th>\n",
       "      <th>AgeGroup_Adult</th>\n",
       "      <th>AgeGroup_Senior</th>\n",
       "      <th>Fare_Unknown</th>\n",
       "      <th>Fare_1_quartile</th>\n",
       "      <th>Fare_2_quartile</th>\n",
       "      <th>Fare_3_quartile</th>\n",
       "      <th>Fare_4_quartile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cabin  Parch  Pclass  Sex  SibSp  Survived  Title_Master  Title_Miss  \\\n",
       "0      0      0       3    0      1         0             0           0   \n",
       "1      1      0       1    1      1         1             0           0   \n",
       "2      0      0       3    1      0         1             0           1   \n",
       "3      1      0       1    1      1         1             0           0   \n",
       "4      0      0       3    0      0         0             0           0   \n",
       "\n",
       "   Title_Mr  Title_Mrs  ...  AgeGroup_Teenager  AgeGroup_Student  \\\n",
       "0         1          0  ...                  0                 1   \n",
       "1         0          1  ...                  0                 0   \n",
       "2         0          0  ...                  0                 0   \n",
       "3         0          1  ...                  0                 0   \n",
       "4         1          0  ...                  0                 0   \n",
       "\n",
       "   AgeGroup_Young Adult  AgeGroup_Adult  AgeGroup_Senior  Fare_Unknown  \\\n",
       "0                     0               0                0             0   \n",
       "1                     0               1                0             0   \n",
       "2                     1               0                0             0   \n",
       "3                     1               0                0             0   \n",
       "4                     1               0                0             0   \n",
       "\n",
       "   Fare_1_quartile  Fare_2_quartile  Fare_3_quartile  Fare_4_quartile  \n",
       "0                1                0                0                0  \n",
       "1                0                0                0                1  \n",
       "2                1                0                0                0  \n",
       "3                0                0                0                1  \n",
       "4                0                1                0                0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Title_Master</th>\n",
       "      <th>Title_Miss</th>\n",
       "      <th>Title_Mr</th>\n",
       "      <th>Title_Mrs</th>\n",
       "      <th>Title_Rare</th>\n",
       "      <th>...</th>\n",
       "      <th>AgeGroup_Teenager</th>\n",
       "      <th>AgeGroup_Student</th>\n",
       "      <th>AgeGroup_Young Adult</th>\n",
       "      <th>AgeGroup_Adult</th>\n",
       "      <th>AgeGroup_Senior</th>\n",
       "      <th>Fare_Unknown</th>\n",
       "      <th>Fare_1_quartile</th>\n",
       "      <th>Fare_2_quartile</th>\n",
       "      <th>Fare_3_quartile</th>\n",
       "      <th>Fare_4_quartile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cabin  Parch  Pclass  Sex  SibSp  Title_Master  Title_Miss  Title_Mr  \\\n",
       "0      0      0       3    0      1             0           0         1   \n",
       "1      1      0       1    1      1             0           0         0   \n",
       "2      0      0       3    1      0             0           1         0   \n",
       "3      1      0       1    1      1             0           0         0   \n",
       "4      0      0       3    0      0             0           0         1   \n",
       "\n",
       "   Title_Mrs  Title_Rare  ...  AgeGroup_Teenager  AgeGroup_Student  \\\n",
       "0          0           0  ...                  0                 1   \n",
       "1          1           0  ...                  0                 0   \n",
       "2          0           0  ...                  0                 0   \n",
       "3          1           0  ...                  0                 0   \n",
       "4          0           0  ...                  0                 0   \n",
       "\n",
       "   AgeGroup_Young Adult  AgeGroup_Adult  AgeGroup_Senior  Fare_Unknown  \\\n",
       "0                     0               0                0             0   \n",
       "1                     0               1                0             0   \n",
       "2                     1               0                0             0   \n",
       "3                     1               0                0             0   \n",
       "4                     1               0                0             0   \n",
       "\n",
       "   Fare_1_quartile  Fare_2_quartile  Fare_3_quartile  Fare_4_quartile  \n",
       "0                1                0                0                0  \n",
       "1                0                0                0                1  \n",
       "2                1                0                0                0  \n",
       "3                0                0                0                1  \n",
       "4                0                1                0                0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 機械学習モデルの構築\n",
    "作成した特徴量を用いて機械学習モデルを学習させます。今回使うアルゴリズムは決定木になります。決定木のハイパーパラメータの中で木の深さと、各ノードでの最小のデータ数についてグリッドサーチで探索して最適化します。探索した中で一番精度のよいハイパーパラメータを持つモデルを活用して未知のデータ（テストデータ)について推論します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "decisiontree = DecisionTreeClassifier()\n",
    "\n",
    "#学習データを正解データと特徴量にわけます\n",
    "X = train.drop([\"Survived\"], axis=1)\n",
    "y = train[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 9 candidates, totalling 36 fits\n",
      "[CV] max_depth=1, min_samples_split=10 ...............................\n",
      "[CV] ................ max_depth=1, min_samples_split=10, total=   0.0s\n",
      "[CV] max_depth=1, min_samples_split=10 ...............................\n",
      "[CV] ................ max_depth=1, min_samples_split=10, total=   0.0s\n",
      "[CV] max_depth=1, min_samples_split=10 ...............................\n",
      "[CV] ................ max_depth=1, min_samples_split=10, total=   0.0s\n",
      "[CV] max_depth=1, min_samples_split=10 ...............................\n",
      "[CV] ................ max_depth=1, min_samples_split=10, total=   0.0s\n",
      "[CV] max_depth=1, min_samples_split=30 ...............................\n",
      "[CV] ................ max_depth=1, min_samples_split=30, total=   0.0s\n",
      "[CV] max_depth=1, min_samples_split=30 ...............................\n",
      "[CV] ................ max_depth=1, min_samples_split=30, total=   0.0s\n",
      "[CV] max_depth=1, min_samples_split=30 ...............................\n",
      "[CV] ................ max_depth=1, min_samples_split=30, total=   0.0s\n",
      "[CV] max_depth=1, min_samples_split=30 ...............................\n",
      "[CV] ................ max_depth=1, min_samples_split=30, total=   0.0s\n",
      "[CV] max_depth=1, min_samples_split=50 ...............................\n",
      "[CV] ................ max_depth=1, min_samples_split=50, total=   0.0s\n",
      "[CV] max_depth=1, min_samples_split=50 ...............................\n",
      "[CV] ................ max_depth=1, min_samples_split=50, total=   0.0s\n",
      "[CV] max_depth=1, min_samples_split=50 ...............................\n",
      "[CV] ................ max_depth=1, min_samples_split=50, total=   0.0s\n",
      "[CV] max_depth=1, min_samples_split=50 ...............................\n",
      "[CV] ................ max_depth=1, min_samples_split=50, total=   0.0s\n",
      "[CV] max_depth=2, min_samples_split=10 ...............................\n",
      "[CV] ................ max_depth=2, min_samples_split=10, total=   0.0s\n",
      "[CV] max_depth=2, min_samples_split=10 ...............................\n",
      "[CV] ................ max_depth=2, min_samples_split=10, total=   0.0s\n",
      "[CV] max_depth=2, min_samples_split=10 ...............................\n",
      "[CV] ................ max_depth=2, min_samples_split=10, total=   0.0s\n",
      "[CV] max_depth=2, min_samples_split=10 ...............................\n",
      "[CV] ................ max_depth=2, min_samples_split=10, total=   0.0s\n",
      "[CV] max_depth=2, min_samples_split=30 ...............................\n",
      "[CV] ................ max_depth=2, min_samples_split=30, total=   0.0s\n",
      "[CV] max_depth=2, min_samples_split=30 ...............................\n",
      "[CV] ................ max_depth=2, min_samples_split=30, total=   0.0s\n",
      "[CV] max_depth=2, min_samples_split=30 ...............................\n",
      "[CV] ................ max_depth=2, min_samples_split=30, total=   0.0s\n",
      "[CV] max_depth=2, min_samples_split=30 ...............................\n",
      "[CV] ................ max_depth=2, min_samples_split=30, total=   0.0s\n",
      "[CV] max_depth=2, min_samples_split=50 ...............................\n",
      "[CV] ................ max_depth=2, min_samples_split=50, total=   0.0s\n",
      "[CV] max_depth=2, min_samples_split=50 ...............................\n",
      "[CV] ................ max_depth=2, min_samples_split=50, total=   0.0s\n",
      "[CV] max_depth=2, min_samples_split=50 ...............................\n",
      "[CV] ................ max_depth=2, min_samples_split=50, total=   0.0s\n",
      "[CV] max_depth=2, min_samples_split=50 ...............................\n",
      "[CV] ................ max_depth=2, min_samples_split=50, total=   0.0s\n",
      "[CV] max_depth=3, min_samples_split=10 ...............................\n",
      "[CV] ................ max_depth=3, min_samples_split=10, total=   0.0s\n",
      "[CV] max_depth=3, min_samples_split=10 ...............................\n",
      "[CV] ................ max_depth=3, min_samples_split=10, total=   0.0s\n",
      "[CV] max_depth=3, min_samples_split=10 ...............................\n",
      "[CV] ................ max_depth=3, min_samples_split=10, total=   0.0s\n",
      "[CV] max_depth=3, min_samples_split=10 ...............................\n",
      "[CV] ................ max_depth=3, min_samples_split=10, total=   0.0s\n",
      "[CV] max_depth=3, min_samples_split=30 ...............................\n",
      "[CV] ................ max_depth=3, min_samples_split=30, total=   0.0s\n",
      "[CV] max_depth=3, min_samples_split=30 ...............................\n",
      "[CV] ................ max_depth=3, min_samples_split=30, total=   0.0s\n",
      "[CV] max_depth=3, min_samples_split=30 ...............................\n",
      "[CV] ................ max_depth=3, min_samples_split=30, total=   0.0s\n",
      "[CV] max_depth=3, min_samples_split=30 ...............................\n",
      "[CV] ................ max_depth=3, min_samples_split=30, total=   0.0s\n",
      "[CV] max_depth=3, min_samples_split=50 ...............................\n",
      "[CV] ................ max_depth=3, min_samples_split=50, total=   0.0s\n",
      "[CV] max_depth=3, min_samples_split=50 ...............................\n",
      "[CV] ................ max_depth=3, min_samples_split=50, total=   0.0s\n",
      "[CV] max_depth=3, min_samples_split=50 ...............................\n",
      "[CV] ................ max_depth=3, min_samples_split=50, total=   0.0s\n",
      "[CV] max_depth=3, min_samples_split=50 ...............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................ max_depth=3, min_samples_split=50, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise-deprecating',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'max_depth': [1, 2, 3], 'min_samples_split': [10, 30, 50]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ハイパーパラメータ探索のためののパラメータグリッドを作成\n",
    "param_grid = {\n",
    "    \"max_depth\":[1, 2, 3],\n",
    "    \"min_samples_split\":[10, 30, 50]}\n",
    "\n",
    "#グリッドサーチで求めるそれぞれのハイパーパラメータの精度はCrossValidation（交差検証）で検証\n",
    "gscv = GridSearchCV(decisiontree, param_grid, cv=4, verbose=2, return_train_score=True, scoring='accuracy')\n",
    "gscv.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交差検証の結果を確認\n",
    "どのハイパーパラメーターが交差検証上で精度が良いかを確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003051</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 10}</td>\n",
       "      <td>0.825893</td>\n",
       "      <td>0.820628</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.828829</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.006848</td>\n",
       "      <td>1</td>\n",
       "      <td>0.823088</td>\n",
       "      <td>0.821856</td>\n",
       "      <td>0.844544</td>\n",
       "      <td>0.822123</td>\n",
       "      <td>0.827903</td>\n",
       "      <td>0.009619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002893</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 30}</td>\n",
       "      <td>0.825893</td>\n",
       "      <td>0.820628</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.828829</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.006848</td>\n",
       "      <td>1</td>\n",
       "      <td>0.823088</td>\n",
       "      <td>0.821856</td>\n",
       "      <td>0.844544</td>\n",
       "      <td>0.822123</td>\n",
       "      <td>0.827903</td>\n",
       "      <td>0.009619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002854</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 50}</td>\n",
       "      <td>0.825893</td>\n",
       "      <td>0.820628</td>\n",
       "      <td>0.806306</td>\n",
       "      <td>0.828829</td>\n",
       "      <td>0.820426</td>\n",
       "      <td>0.008648</td>\n",
       "      <td>4</td>\n",
       "      <td>0.823088</td>\n",
       "      <td>0.821856</td>\n",
       "      <td>0.822123</td>\n",
       "      <td>0.822123</td>\n",
       "      <td>0.822297</td>\n",
       "      <td>0.000469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003118</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': 5, 'min_samples_split': 10}</td>\n",
       "      <td>0.741071</td>\n",
       "      <td>0.847534</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.801802</td>\n",
       "      <td>0.800224</td>\n",
       "      <td>0.038320</td>\n",
       "      <td>9</td>\n",
       "      <td>0.845577</td>\n",
       "      <td>0.847305</td>\n",
       "      <td>0.862481</td>\n",
       "      <td>0.855007</td>\n",
       "      <td>0.852593</td>\n",
       "      <td>0.006723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002982</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>{'max_depth': 5, 'min_samples_split': 30}</td>\n",
       "      <td>0.763393</td>\n",
       "      <td>0.838565</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.806306</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.026943</td>\n",
       "      <td>7</td>\n",
       "      <td>0.835082</td>\n",
       "      <td>0.833832</td>\n",
       "      <td>0.856502</td>\n",
       "      <td>0.843049</td>\n",
       "      <td>0.842117</td>\n",
       "      <td>0.009027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.003088</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>{'max_depth': 5, 'min_samples_split': 50}</td>\n",
       "      <td>0.767857</td>\n",
       "      <td>0.838565</td>\n",
       "      <td>0.801802</td>\n",
       "      <td>0.806306</td>\n",
       "      <td>0.803591</td>\n",
       "      <td>0.025096</td>\n",
       "      <td>8</td>\n",
       "      <td>0.832084</td>\n",
       "      <td>0.830838</td>\n",
       "      <td>0.829596</td>\n",
       "      <td>0.843049</td>\n",
       "      <td>0.833892</td>\n",
       "      <td>0.005360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.003376</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_split': 10}</td>\n",
       "      <td>0.808036</td>\n",
       "      <td>0.825112</td>\n",
       "      <td>0.815315</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.820426</td>\n",
       "      <td>0.009597</td>\n",
       "      <td>4</td>\n",
       "      <td>0.887556</td>\n",
       "      <td>0.877246</td>\n",
       "      <td>0.881913</td>\n",
       "      <td>0.883408</td>\n",
       "      <td>0.882531</td>\n",
       "      <td>0.003686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.003240</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_split': 30}</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.838565</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.011797</td>\n",
       "      <td>6</td>\n",
       "      <td>0.857571</td>\n",
       "      <td>0.848802</td>\n",
       "      <td>0.862481</td>\n",
       "      <td>0.847534</td>\n",
       "      <td>0.854097</td>\n",
       "      <td>0.006194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.003401</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_split': 50}</td>\n",
       "      <td>0.825893</td>\n",
       "      <td>0.838565</td>\n",
       "      <td>0.801802</td>\n",
       "      <td>0.819820</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.013233</td>\n",
       "      <td>1</td>\n",
       "      <td>0.850075</td>\n",
       "      <td>0.839820</td>\n",
       "      <td>0.829596</td>\n",
       "      <td>0.844544</td>\n",
       "      <td>0.841009</td>\n",
       "      <td>0.007522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.003051      0.000154         0.001092        0.000015   \n",
       "1       0.002893      0.000090         0.001088        0.000009   \n",
       "2       0.002854      0.000107         0.001104        0.000023   \n",
       "3       0.003118      0.000052         0.001082        0.000001   \n",
       "4       0.002982      0.000093         0.001106        0.000012   \n",
       "5       0.003088      0.000075         0.001093        0.000013   \n",
       "6       0.003376      0.000124         0.001124        0.000029   \n",
       "7       0.003240      0.000050         0.001096        0.000017   \n",
       "8       0.003401      0.000424         0.001088        0.000011   \n",
       "\n",
       "  param_max_depth param_min_samples_split  \\\n",
       "0               3                      10   \n",
       "1               3                      30   \n",
       "2               3                      50   \n",
       "3               5                      10   \n",
       "4               5                      30   \n",
       "5               5                      50   \n",
       "6              10                      10   \n",
       "7              10                      30   \n",
       "8              10                      50   \n",
       "\n",
       "                                       params  split0_test_score  \\\n",
       "0   {'max_depth': 3, 'min_samples_split': 10}           0.825893   \n",
       "1   {'max_depth': 3, 'min_samples_split': 30}           0.825893   \n",
       "2   {'max_depth': 3, 'min_samples_split': 50}           0.825893   \n",
       "3   {'max_depth': 5, 'min_samples_split': 10}           0.741071   \n",
       "4   {'max_depth': 5, 'min_samples_split': 30}           0.763393   \n",
       "5   {'max_depth': 5, 'min_samples_split': 50}           0.767857   \n",
       "6  {'max_depth': 10, 'min_samples_split': 10}           0.808036   \n",
       "7  {'max_depth': 10, 'min_samples_split': 30}           0.812500   \n",
       "8  {'max_depth': 10, 'min_samples_split': 50}           0.825893   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
       "0           0.820628           0.810811           0.828829         0.821549   \n",
       "1           0.820628           0.810811           0.828829         0.821549   \n",
       "2           0.820628           0.806306           0.828829         0.820426   \n",
       "3           0.847534           0.810811           0.801802         0.800224   \n",
       "4           0.838565           0.810811           0.806306         0.804714   \n",
       "5           0.838565           0.801802           0.806306         0.803591   \n",
       "6           0.825112           0.815315           0.833333         0.820426   \n",
       "7           0.838565           0.810811           0.810811         0.818182   \n",
       "8           0.838565           0.801802           0.819820         0.821549   \n",
       "\n",
       "   std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0        0.006848                1            0.823088            0.821856   \n",
       "1        0.006848                1            0.823088            0.821856   \n",
       "2        0.008648                4            0.823088            0.821856   \n",
       "3        0.038320                9            0.845577            0.847305   \n",
       "4        0.026943                7            0.835082            0.833832   \n",
       "5        0.025096                8            0.832084            0.830838   \n",
       "6        0.009597                4            0.887556            0.877246   \n",
       "7        0.011797                6            0.857571            0.848802   \n",
       "8        0.013233                1            0.850075            0.839820   \n",
       "\n",
       "   split2_train_score  split3_train_score  mean_train_score  std_train_score  \n",
       "0            0.844544            0.822123          0.827903         0.009619  \n",
       "1            0.844544            0.822123          0.827903         0.009619  \n",
       "2            0.822123            0.822123          0.822297         0.000469  \n",
       "3            0.862481            0.855007          0.852593         0.006723  \n",
       "4            0.856502            0.843049          0.842117         0.009027  \n",
       "5            0.829596            0.843049          0.833892         0.005360  \n",
       "6            0.881913            0.883408          0.882531         0.003686  \n",
       "7            0.862481            0.847534          0.854097         0.006194  \n",
       "8            0.829596            0.844544          0.841009         0.007522  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(gscv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 10}</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.006848</td>\n",
       "      <td>0.827903</td>\n",
       "      <td>0.009619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 30}</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.006848</td>\n",
       "      <td>0.827903</td>\n",
       "      <td>0.009619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 50}</td>\n",
       "      <td>0.820426</td>\n",
       "      <td>0.008648</td>\n",
       "      <td>0.822297</td>\n",
       "      <td>0.000469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'max_depth': 5, 'min_samples_split': 10}</td>\n",
       "      <td>0.801347</td>\n",
       "      <td>0.036597</td>\n",
       "      <td>0.852967</td>\n",
       "      <td>0.007281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'max_depth': 5, 'min_samples_split': 30}</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.026943</td>\n",
       "      <td>0.842117</td>\n",
       "      <td>0.009027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'max_depth': 5, 'min_samples_split': 50}</td>\n",
       "      <td>0.803591</td>\n",
       "      <td>0.025096</td>\n",
       "      <td>0.833892</td>\n",
       "      <td>0.005360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'max_depth': 10, 'min_samples_split': 10}</td>\n",
       "      <td>0.822671</td>\n",
       "      <td>0.015884</td>\n",
       "      <td>0.881781</td>\n",
       "      <td>0.002781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'max_depth': 10, 'min_samples_split': 30}</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.011797</td>\n",
       "      <td>0.854097</td>\n",
       "      <td>0.006194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'max_depth': 10, 'min_samples_split': 50}</td>\n",
       "      <td>0.820426</td>\n",
       "      <td>0.013004</td>\n",
       "      <td>0.840259</td>\n",
       "      <td>0.006684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       params  mean_test_score  \\\n",
       "0   {'max_depth': 3, 'min_samples_split': 10}         0.821549   \n",
       "1   {'max_depth': 3, 'min_samples_split': 30}         0.821549   \n",
       "2   {'max_depth': 3, 'min_samples_split': 50}         0.820426   \n",
       "3   {'max_depth': 5, 'min_samples_split': 10}         0.801347   \n",
       "4   {'max_depth': 5, 'min_samples_split': 30}         0.804714   \n",
       "5   {'max_depth': 5, 'min_samples_split': 50}         0.803591   \n",
       "6  {'max_depth': 10, 'min_samples_split': 10}         0.822671   \n",
       "7  {'max_depth': 10, 'min_samples_split': 30}         0.818182   \n",
       "8  {'max_depth': 10, 'min_samples_split': 50}         0.820426   \n",
       "\n",
       "   std_test_score  mean_train_score  std_train_score  \n",
       "0        0.006848          0.827903         0.009619  \n",
       "1        0.006848          0.827903         0.009619  \n",
       "2        0.008648          0.822297         0.000469  \n",
       "3        0.036597          0.852967         0.007281  \n",
       "4        0.026943          0.842117         0.009027  \n",
       "5        0.025096          0.833892         0.005360  \n",
       "6        0.015884          0.881781         0.002781  \n",
       "7        0.011797          0.854097         0.006194  \n",
       "8        0.013004          0.840259         0.006684  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(gscv.cv_results_)[[\"params\", \"mean_test_score\", \"std_test_score\", \"mean_train_score\", \"std_train_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一番良かったハイパーパラメーターは {'max_depth': 3, 'min_samples_split': 10} でAccuracyスコアは 0.822。\n"
     ]
    }
   ],
   "source": [
    "print(\"一番良かったハイパーパラメーターは {} でAccuracyスコアは {:.3g}。\".format(gscv.best_params_, gscv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推論\n",
    "学習データでの交差検証で一番精度が良かったモデルを用いてテストデータを推論します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=5,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n"
     ]
    }
   ],
   "source": [
    "best_decisiontree = gscv.best_estimator_\n",
    "print(best_decisiontree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_decisiontree.predict(test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ご参考\n",
    "- [Python機械学習ライブラリ scikit-learn活用レシピ80+](https://www.amazon.co.jp/dp/4295005746)\n",
    "- [Pythonデータ分析/機械学習のための基本コーディング! pandasライブラリ活用入門 ](https://www.amazon.co.jp/dp/4295005657)\n",
    "- [機械学習のための特徴量エンジニアリング](https://www.amazon.co.jp/dp/4873118689)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
