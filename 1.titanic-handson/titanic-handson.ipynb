{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## はじめに\n",
    "\n",
    "本ハンズオンでは機械学習の基礎を体験頂きます。コードの詳細は気にせずに、どういった手順で何をしようとしているのか、機械学習モデル構築の流れをご確認下さい。また不明な単語などありましたら、参考文献などでご確認を頂けたらと思います。\n",
    "\n",
    "今回は機械学習モデルのチュートリアルでは定番のTitanicのデータを活用します。\n",
    "\n",
    "今後のプロトタイピングを実施するに辺り、本ハンズオンを通してご確認頂きたい概念は下記です。\n",
    "- 教師あり学習とは\n",
    "- 探索的データ解析(EDA)から特徴量を作成する流れ\n",
    "- 欠損データへの対応\n",
    "- カテゴリカル変数の取り扱い(ダミー変数化など)\n",
    "- 機械学習アルゴリズムのハイパーパラメータとは\n",
    "- 交差検証とは\n",
    "\n",
    "逆に、現時点で時間をかけて頂かなくて良い点は下記になります。\n",
    "- 機械学習アルゴリズムそのもの(今回で言えば決定木アルゴリズム）\n",
    "- Pythonライブラリのそれぞれの詳細(numpy, pandas, scikit-learnなど)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://bit.ly/2KEZlRT\n",
    "!unzip -o 2KEZlRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの確認\n",
    "まずは元データについて確認をします。今回は簡単にデータサイズ、カラム名、サンプルの抽出を実施します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリーのインポート\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#データの読み込み\n",
    "train = pd.read_csv(\"titanic-data/train.csv\")\n",
    "test = pd.read_csv(\"titanic-data/test.csv\")\n",
    "\n",
    "#データセットのサイズの確認\n",
    "print(\"=====元データのサイズは下記====\")\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "\n",
    "#データセットのカラム名の確認\n",
    "print(\"=====元データのカラムは下記====\")\n",
    "print(train.columns)\n",
    "\n",
    "#どのようなデータセットになっているのか、サンプルを5個抽出\n",
    "train.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回のデータセットで変数の種類はそれぞれ下記となっています。\n",
    "- Boolian: Survived\n",
    "- Numerical: Age (連続値), Fare (連続値), SibSp (離散値), Parch (離散値)\n",
    "- Categorical: Survived, Sex, Embarked, Pclass\n",
    "- Alphanumeric: Ticket, Cabin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 欠損値の確認\n",
    "今回のデータはいくつかの特徴量において欠損があるためそれぞれに対応する必要があります。先程のサンプルでも`Cabin`の各データは全て\"NaN\"で欠損しています。他の特徴にも欠損が無いか見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#欠損値の確認\n",
    "print(pd.isnull(train).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA(Exploratory Data Analysis)：探索的データ解析\n",
    "EDAの結果、下記のようなことがわかります。\n",
    "\n",
    "- 今回のデータは学習データに891人、テストデータに418人の乗客がそれぞれいる\n",
    "- Sex: 女性の方が生存しやすい\n",
    "- SibSp/Parch: 一人で旅行している乗客ほど生存しやすい\n",
    "- Age: 幼い子どもほど生存しやすい\n",
    "- Pclass: Pclassの高い乗客ほど生存しやすい\n",
    "- 約2割のデータで年齢が欠損しています。年齢によって生存率が大きく異なる\n",
    "- キャビンについては約8割のデータが欠損している\n",
    "\n",
    "その他にも今回のデータセットを確認すると様々なことがわかります。本ハンズオンでは詳細を確認しませんので、ご興味ある方が、他の方々が実施されている内容をご参考に下さい。\n",
    "- [Scikit-Learn ML from Start to Finish](https://www.kaggle.com/jeffd23/scikit-learn-ml-from-start-to-finish)\n",
    "- [Titanic Survival Predictions (Beginner)](https://www.kaggle.com/nadintamer/titanic-survival-predictions-beginner)\n",
    "- [Titanic top 10% - My first competition](https://www.kaggle.com/benteo/titanic-top-10-my-first-competition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ前処理と特徴量作成\n",
    "ここから先は現在あるデータを用いて機械学習モデルの精度が向上するようなデータを作成します。\n",
    "- 欠損しているデータを活用できる形へ変換したり埋めたりする\n",
    "- 機械学習モデルにとってより意味のある特徴量を作成する\n",
    "- カテゴリカル変数を機械学習モデルへ投入できるようなダミー変数へと変換する\n",
    "\n",
    "下記の方針で前処理と特徴量作成を実施します。\n",
    "- Cabin：記録が残っている乗客の方が生存率が高いため、記録あり、無し(欠損)、を特徴量とする\n",
    "- Embarked：乗客の多くがSouthamptonから乗船しているので、記載が無い乗客もSouthamptonから乗船したことにして欠損埋めをする\n",
    "- Age：年齢が若いほうが生存が高いが、欠損が多いため活用が難しい。他のデータから年齢を推測した上で、グループ化して活用\n",
    "    - `Name`特徴量中から`Title`を抽出\n",
    "    - `Title`から年齢がどの程度が推察し`AgeGroup`特徴量を作成\n",
    "- Fare：欠損値を適当な値で埋めた後に四分位毎にグループ化\n",
    "\n",
    "最終的に機械学習モデルが学習できるようカテゴリカル変数をダミー変数化します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cabin特徴量\n",
    "`Cabin`に記録が残っている乗客の方が生存率が高いため、記録あり、無し(欠損)、を特徴量とする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"Cabin\"] = train[\"Cabin\"].notnull().astype('int')\n",
    "test[\"Cabin\"] = test[\"Cabin\"].notnull().astype('int')\n",
    "\n",
    "#作った特徴量の確認\n",
    "train[\"Cabin\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embarked特徴量\n",
    "乗客の多くがSouthamptonから乗船しているので、記載が無い乗客もSouthamptonから乗船したこととして欠損埋めを実施。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Southampton(S)からの乗客数:\" + str(train[train[\"Embarked\"] == \"S\"].shape[0]))\n",
    "print(\"Cherbourg(C)からの乗客数:\" + str(train[train[\"Embarked\"] == \"C\"].shape[0]))\n",
    "print(\"Queenstown(Q)からの乗客数:\"+ str(train[train[\"Embarked\"] == \"Q\"].shape[0]))\n",
    "\n",
    "train = train.fillna({\"Embarked\": \"S\"})\n",
    "test = train.fillna({\"Embarked\": \"S\"})\n",
    "\n",
    "#作った特徴量の確認\n",
    "train[\"Embarked\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age特徴量\n",
    "元のデータは多くの人の年齢情報が欠損しています。今回は名前についている`Title`から年齢を推測して活用しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#年齢をそのまま使わずにグループに変換\n",
    "train[\"Age\"] = train[\"Age\"].fillna(-0.5)\n",
    "test[\"Age\"] = test[\"Age\"].fillna(-0.5)\n",
    "bins = [-1, 0, 5, 12, 18, 24, 35, 60, np.inf]\n",
    "labels = [\"Unknown\", \"Baby\", \"Child\", \"Teenager\", \"Student\", \"Young Adult\", \"Adult\", \"Senior\"]\n",
    "train[\"AgeGroup\"] = pd.cut(train[\"Age\"], bins, labels = labels)\n",
    "test[\"AgeGroup\"] = pd.cut(test[\"Age\"], bins, labels = labels)\n",
    "\n",
    "#学習データとテストデータの`Name`特徴量から`Title`部分を抽出\n",
    "combine = [train, test]\n",
    "for dataset in combine:\n",
    "    dataset[\"Title\"] = dataset.Name.str.extract(\" ([A-Za-z]+)\\.\", expand=False)\n",
    "    \n",
    "#様々なタイトルをより一般的なタイトルへ変換します\n",
    "for dataset in combine:\n",
    "    dataset[\"Title\"] = dataset[\"Title\"].replace([\"Lady\", \"Capt\", \"Col\",\n",
    "    \"Don\", \"Dr\", \"Major\", \"Rev\", \"Jonkheer\", \"Dona\"], \"Rare\")\n",
    "    \n",
    "    dataset[\"Title\"] = dataset[\"Title\"].replace([\"Countess\", \"Lady\", \"Sir\"], \"Royal\")\n",
    "    dataset[\"Title\"] = dataset[\"Title\"].replace(\"Mlle\", \"Miss\")\n",
    "    dataset[\"Title\"] = dataset[\"Title\"].replace(\"Ms\", \"Miss\")\n",
    "    dataset[\"Title\"] = dataset[\"Title\"].replace(\"Mme\", \"Mrs\")\n",
    "    dataset[\"Title\"] = dataset[\"Title\"].fillna(\"NaN\")\n",
    "\n",
    "#作った特徴量の確認\n",
    "print(train[\"Title\"].head())\n",
    "\n",
    "train[[\"Title\", \"Survived\"]].groupby([\"Title\"], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Mr`がついている方の生存率が極端に低い一方、`Miss`や`Mrs`の生存率が高いことがわかります。Royalは確実に生き残っているところには感じ入るものがあります。指名から抽出したタイトルをもとにして、どの年齢カテゴリなのかを推察します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_title_mapping = {\"Mr\": \"Young Adult\", \"Miss\": \"Student\", \"Mrs\": \"Adult\", \"Master\": \"Baby\", \"Royal\": \"Adult\", \"Rare\": \"Adult\"}\n",
    "\n",
    "for x in range(len(train[\"AgeGroup\"])):\n",
    "    if train[\"AgeGroup\"][x] == \"Unknown\":\n",
    "        train[\"AgeGroup\"][x] = age_title_mapping[train[\"Title\"][x]]\n",
    "        \n",
    "for x in range(len(test[\"AgeGroup\"])):\n",
    "    if test[\"AgeGroup\"][x] == \"Unknown\":\n",
    "        test[\"AgeGroup\"][x] = age_title_mapping[test[\"Title\"][x]]\n",
    "        \n",
    "        \n",
    "#作った特徴量の確認        \n",
    "train[\"AgeGroup\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fare特徴量\n",
    "欠損値を欠損していたことがわかりやすい適当な値で埋めた上で、四分位毎でビンで区切って離散化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"Fare\"] = train[\"Fare\"].fillna(-0.5)\n",
    "test[\"Fare\"] = train[\"Fare\"].fillna(-0.5)\n",
    "bins = (-1, 0, 8, 15, 31, 1000)\n",
    "\n",
    "group_names = [\"Unknown\", \"1_quartile\", \"2_quartile\", \"3_quartile\", \"4_quartile\"]\n",
    "\n",
    "train[\"Fare\"] = pd.cut(train[\"Fare\"], bins, labels=group_names)\n",
    "test[\"Fare\"] = pd.cut(test[\"Fare\"], bins, labels=group_names)\n",
    "\n",
    "#作った特徴量の確認\n",
    "train[\"Fare\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ダミー変数化(ダミーエンコーディング)\n",
    "カテゴリカル変数をダミー変数化(ダミーエンコーディング)します。対象となるのは、`Title`、`Embarked`、`AgeGroup`、`Fare`、`Sex`です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#処理しやすいように学習データとテストデータを結合\n",
    "data = pd.concat([train,test], axis=0, sort=\"True\")\n",
    "\n",
    "title_dummies = pd.get_dummies(data[\"Title\"], prefix=\"Title\")\n",
    "data = pd.concat([data, title_dummies], axis=1)\n",
    "data = data.drop([\"Title\"], axis=1)\n",
    "\n",
    "embarked_dummies = pd.get_dummies(data[\"Embarked\"], prefix=\"Embarked\")\n",
    "data = pd.concat([data, embarked_dummies], axis=1)\n",
    "data = data.drop([\"Embarked\"], axis=1)\n",
    "\n",
    "agegroup_dummies = pd.get_dummies(data[\"AgeGroup\"], prefix=\"AgeGroup\")\n",
    "data = pd.concat([data, agegroup_dummies], axis=1)\n",
    "data = data.drop([\"AgeGroup\"], axis=1)\n",
    "\n",
    "fare_dummies = pd.get_dummies(data[\"Fare\"], prefix=\"Fare\")\n",
    "data = pd.concat([data, fare_dummies], axis=1)\n",
    "data = data.drop([\"Fare\"], axis=1)\n",
    "\n",
    "sex_mapping = {\"male\": 0, \"female\": 1}\n",
    "data[\"Sex\"] = data[\"Sex\"].map(sex_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特徴量の削除\n",
    "あまり有用な情報がなかったり特徴量作成のベースとして使った特徴量を削除します。対象となるのは`Age`、`Name`、`Ticket`、`PassengerId`です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使わない特徴量の削除\n",
    "data = data.drop([\"Age\", \"Ticket\", \"Name\", \"PassengerId\"], axis = 1)\n",
    "\n",
    "#結合したデータを再び学習データとテストデータへ分離\n",
    "train = data.iloc[:len(train)]\n",
    "test = data.iloc[-len(test):].drop([\"Survived\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの確認\n",
    "最終的にモデル構築に使うデータを確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 機械学習モデルの構築\n",
    "作成した特徴量を用いて機械学習モデルを学習させます。今回使うアルゴリズムは決定木になります。決定木のハイパーパラメータの中で木の深さと、各ノードでの最小のデータ数についてグリッドサーチで探索して最適化します。探索した中で一番精度のよいハイパーパラメータを持つモデルを活用して未知のデータ（テストデータ)について推論します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "decisiontree = DecisionTreeClassifier()\n",
    "\n",
    "#学習データを正解データと特徴量にわけます\n",
    "X = train.drop([\"Survived\"], axis=1)\n",
    "y = train[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ハイパーパラメータ探索のためののパラメータグリッドを作成\n",
    "param_grid = {\n",
    "    \"max_depth\":[3, 5, 7],\n",
    "    \"min_samples_split\":[10, 30, 50]}\n",
    "\n",
    "#グリッドサーチで求めるそれぞれのハイパーパラメータの精度はCrossValidation（交差検証）で検証\n",
    "gscv = GridSearchCV(decisiontree, param_grid, cv=4, verbose=2, return_train_score=True, scoring='accuracy')\n",
    "gscv.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交差検証の結果を確認\n",
    "どのハイパーパラメーターが交差検証上で精度が良いかを確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交差検証の結果をDataFrameで表示します。\n",
    "pd.DataFrame.from_dict(gscv.cv_results_)[[\"params\", \"mean_test_score\", \"std_test_score\", \"mean_train_score\", \"std_train_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"一番良かったハイパーパラメーターは {} でAccuracyスコアは {:.3g}。\".format(gscv.best_params_, gscv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推論\n",
    "学習データでの交差検証で一番精度が良かったモデルを用いてテストデータを推論します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_decisiontree.predict(test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ご参考\n",
    "- [Python機械学習ライブラリ scikit-learn活用レシピ80+](https://www.amazon.co.jp/dp/4295005746)\n",
    "- [Pythonデータ分析/機械学習のための基本コーディング! pandasライブラリ活用入門 ](https://www.amazon.co.jp/dp/4295005657)\n",
    "- [機械学習のための特徴量エンジニアリング](https://www.amazon.co.jp/dp/4873118689)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
