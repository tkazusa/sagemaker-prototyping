{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## はじめに\n",
    "\n",
    "本ハンズオンでは機械学習モデルの解釈性をテーマに、どの特徴量がどの程度の影響があるかについて解析する手法をご体感頂きます。実際の本番環境ではデータの量や計算リソースが異なるため活用するツールが異なることが想定されるため、今回のハンズオンのコードの詳細は気にせずに、どういった手順で何をしようとしているのか、特徴量の寄与度を求める流れをご確認下さい。また不明な単語などありましたら、参考文献などでご確認を頂けたらと思います。\n",
    "\n",
    "今回は[岩波データサイエンス vol.3【特集】因果推論 ― 実世界のデータから因果を読む](https://sites.google.com/site/iwanamidatascience/vol-3/vol3-ingasuiron)で活用されているの[データ](https://github.com/iwanami-datascience/vol3)を活用します。\n",
    "\n",
    "データの詳細については下記をご参考に下さい。\n",
    "- [加藤・星野　サポートページ（岩波データサイエンスVol.3 特集 因果推論）](https://sites.google.com/site/iwanamidatascience/vol-3/vol3-ingasuiron/vol3-katouhoshino)\n",
    "- [データアナリストが活用できる（かもしれない）機械学習](https://speakerdeck.com/nekoumei/detaanarisutogahuo-yong-dekiru-kamosirenai-ji-jie-xue-xi)\n",
    "- [岩波データサイエンスvol.3のデータで遊ぼう](https://rstudio-pubs-static.s3.amazonaws.com/192092_6a8bac32eac24d7ca3a4791a80e7f3d3.html)\n",
    "\n",
    "\n",
    "今後のプロトタイピングを実施するに辺り、本ハンズオンを通してご確認頂きたい概念は下記です。\n",
    "- Tree系アルゴリズムにおける特徴量重要度とその取扱について\n",
    "- 特徴量重要度を用いた特徴量の目的変数への寄与について\n",
    "    - 素の特徴量重要度の活用\n",
    "    - Permutation Importanceの活用\n",
    "- 任意の特徴量を変化させた時の目的変数の変化について\n",
    "    - Partianl Dependency Plot(PDP)の活用\n",
    "- SageMakerでのバッチ推論について\n",
    "\n",
    "もしご興味がある方は下記についてもご確認頂けると今後の役に立つやもしれません。\n",
    "- GDBTやXGBoostとは\n",
    "- 木系アルゴリズムにおける特徴量重要度の計算方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge -y xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define IAM role\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.session import s3_input\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの準備\n",
    "今回は[岩波データサイエンス vol.3【特集】因果推論 ― 実世界のデータから因果を読む](https://sites.google.com/site/iwanamidatascience/vol-3/vol3-ingasuiron)で活用されている、アプリの利用状況とユーザーの属性などについてまとめた[データ](https://github.com/iwanami-datascience/vol3)を活用します。\n",
    "\n",
    "既にダミー変数化などは実施されておりこのまま機械学習アルゴリズムの学習に使える形で提供提供されています。\n",
    "それぞれの特徴量が何を表すかについては[こちらのPDFファイル](https://drive.google.com/file/d/0Bw-J75fYQ33NV19UM3JFLTVJX2s/view)をご参考に頂くと良いです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/iwanami-datascience/vol3/master/kato%26hoshino/q_data_x.csv'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# SageMakerの組み込みアルゴリズムへの対応のためデータの並び替え\n",
    "data = pd.concat([data[\"gamesecond\"], data.drop([\"gamedummy\", \"gamecount\", \"gamesecond\"], axis=1)], axis=1)\n",
    "\n",
    "# データの簡単な確認\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの分割\n",
    "データを学習用、検証用、テスト用データへ分割します。学習＆検証用データで構築したモデルに対してテストデータで推論と、その予測値に対してどの特徴量がどの程度寄与しているか、または変化した際に、どの程度影響を及ぼすのかを解析します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用データ、検証用データ、テスト用データへ分割\n",
    "train_data, validation_data, test_data = np.split(data.sample(frac=1, random_state=1729), [int(0.7 * len(data)), int(0.9 * len(data))])\n",
    "\n",
    "# 学習用データ、検証用データの保存\n",
    "train_data.to_csv(\"train.csv\", header=False, index=False)\n",
    "validation_data.to_csv(\"validation.csv\", header=False, index=False)\n",
    "\n",
    "test_features = test_data.drop([\"gamesecond\"], axis=1)\n",
    "test_true = test_data[\"gamesecond\"]\n",
    "test_features.to_csv(\"test.csv\", header=False, index=False)\n",
    "\n",
    "\n",
    "# アップロード先を指定\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'sagemaker/feature_importance' \n",
    "\n",
    "train_channel = prefix + '/train'\n",
    "validate_channel = prefix + '/validate'\n",
    "test_channel = prefix + '/test'\n",
    "\n",
    "\n",
    "# csvファイルとして保存された学習用データ、検証用データをS3バケット上にアップロード\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "input_train = sagemaker_session.upload_data(path=\"train.csv\", bucket=bucket, key_prefix=train_channel)\n",
    "input_validation = sagemaker_session.upload_data(path=\"validation.csv\", bucket=bucket, key_prefix=validate_channel)\n",
    "input_test = sagemaker_session.upload_data(path=\"test.csv\", bucket=bucket, key_prefix=test_channel)\n",
    "\n",
    "# アップロードされたデータ\n",
    "s3_input_train = s3_input(s3_data=input_train, content_type='text/csv')\n",
    "s3_input_validation = s3_input(s3_data=input_validation, content_type='text/csv')\n",
    "s3_input_test = s3_input(s3_data=input_test, content_type='text/csv')\n",
    "\n",
    "\n",
    "# アップロード先のパス\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n",
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  木系アルゴリズムにおける特徴量重要度とその取扱について\n",
    "特徴量重要度とは木系アルゴリズムにおいて、学習の過程においてどの特徴量がどの程度予測精度に寄与しているかを計算したものです。\n",
    "\n",
    "特徴量の寄与度や目的変数への影響を考慮する場合、基本的には一つの特徴量に注目すると思います。しかし特徴量間に相関がある場合、例えば今回の例で言えばinc(年間収入)とjob_dummy_1(正社員、公務員)で0.72の相関があるなどの場合、どちらがどの程度影響を及ぼしているのか必ずしも明確に判断できない場合があります。そのため機械学習モデル構築とその寄与度を把握する前のEDAの段階で相関を見るようにします。下記では、sexとincやageとM3などで相関が少し強めに見られているので注意が必要そうです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相関行列を描く\n",
    "corr_df = data.corr()\n",
    "plt.figure(figsize=[30, 20])\n",
    "sns.heatmap(corr_df, cmap=\"YlGnBu\", annot=True, fmt='.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 機械学習モデルの構築\n",
    "特徴量重要度を取得するためにSageMakerのxgboost組み込みアルゴリズムを学習させます。今回は独自には特徴量作成をせずに既にある特徴量を用いてデータを学習させます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = get_image_uri(boto3.Session().region_name, 'xgboost')\n",
    "\n",
    "# 学習ジョブの名前を指定して下さい\n",
    "job_name=\"xgb-gamesecond-XXXX\"\n",
    "\n",
    "# どのような学習インスタンスを使うのかの設定\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type=\"ml.m4.4xlarge\",\n",
    "                                    sagemaker_session=sess,\n",
    "                                    output_path=s3_output_location)\n",
    "\n",
    "# XGBoostアルゴリズムのハイパラ設定\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective=\"reg:linear\",\n",
    "                        num_round=100)\n",
    "\n",
    "# 学習の開始\n",
    "xgb.fit({\"train\": s3_input_train, \"validation\": s3_input_validation},  job_name=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習させたモデルから素の特徴量重要度を取得\n",
    "学習させたXGBoostのモデルから特徴量重要度を読み込んで描写します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習させたxgboostモデルのダウンロード\n",
    "s3 = boto3.client('s3')\n",
    "s3.download_file(Bucket=bucket, Key= prefix + \"/output/\" + job_name + \"/output/model.tar.gz\", Filename = 'model.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboostモデルの解凍\n",
    "!tar -zxvf model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ダウンロードしてきたxgboostモデルの読み込み\n",
    "bst = pkl.load(open(\"xgboost-model\", 'rb'))\n",
    "\n",
    "# 特徴量重要度を読み込むためのデータフレームの準備\n",
    "dict_varImp = bst.get_score(importance_type = 'weight')\n",
    "df_ = pd.DataFrame(dict_varImp, index = ['varImp']).transpose().reset_index()\n",
    "df_.columns = ['feature', 'fscore']\n",
    "\n",
    "# 上位10個の特徴量重要度を描写\n",
    "df_['fscore'] = df_['fscore'] / df_['fscore'].max()\n",
    "df_.sort_values('fscore', ascending = False, inplace = True)\n",
    "df_ = df_[0:10]\n",
    "df_.sort_values('fscore', ascending = True, inplace = True)\n",
    "\n",
    "fscore = df_['fscore']\n",
    "feature = df_['feature']\n",
    "\n",
    "mapper = {'f{0}'.format(i): v for i, v in enumerate(data.columns.drop(\"gamesecond\"))}\n",
    "mapped_feature = [mapper[f] for f in df_[\"feature\"]]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('xgboost feature importance', fontsize = 20)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.barh(mapped_feature,fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation importanceの活用\n",
    "任意の特徴量をランダムに並べ替えると、そのデータは実際の世界で観測されるものと異なるため、推論時にモデルの予測精度が低下します。\n",
    "予測に重要な特徴量をシャッフルした場合には特に精度の落ち込みが激しくなるはずです。この考え方に基づいた特徴量の目的変数への寄与度の算出方法がPErmutation importanceです。 \n",
    "手順は下記になります。\n",
    "\n",
    "- テスト用データを推論し本来の目的変数の値(今回はゲームの使用時間)と予測値のRSME(平均二乗誤差)を計算しベーススコアとします。\n",
    "- テスト用のデータの任意の特徴量のみをランダムに並び替えます。\n",
    "- 並び変えたデータを学習済モデルへ並び替えたデータ(Permuted data)で推論します。\n",
    "- 本来の目的変数の値(今回はゲームの使用時間)と並べ替えられた予測値のRSME(平均二乗誤差)を計算します。\n",
    "- ベーススコアと並べ変えた場合のスコアを比較して、その特徴量のPermutated importanceとします。\n",
    "\n",
    "先程と同様にimportanceという単語が使われていますが重要度に使っている指標が異なります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ベーススコアの計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# バッチ変換用のインスタンスを設定\n",
    "xgb_transformer = xgb.transformer(instance_count=1,\n",
    "                                  instance_type='ml.m4.4xlarge',\n",
    "                                  strategy='MultiRecord',\n",
    "                                  assemble_with='Line',\n",
    "                                  output_path=s3_output_location)\n",
    "\n",
    "# バッチ変換用ジョブを設定\n",
    "xgb_transformer.transform(data=input_test, content_type='text/csv', split_type='Line')\n",
    "\n",
    "# バッチ変換の開始\n",
    "xgb_transformer.wait()\n",
    "\n",
    "# 手元でベーススコアを計算するためバッチ変換データのダウンロード\n",
    "s3 = boto3.client('s3')\n",
    "s3.download_file(Bucket=bucket, Key= prefix + \"/output/\" + \"test.csv.out\", Filename = \"test.csv.out\")\n",
    "\n",
    "# テストデータでの本来の値と予測値を読み込み\n",
    "test_true = test_data[\"gamesecond\"]\n",
    "test_predict = pd.read_csv(\"test.csv.out\", names=[\"predict\"])\n",
    "\n",
    "# 本来の値と予測値の平均二乗誤差を計算\n",
    "base_score = mean_squared_error(test_true, test_predict)\n",
    "print(\"ベーススコアは {} です\".format(base_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation importanceの計算\n",
    "今回は例として「TVWatch_day」の特徴量をデータごとにシャッフルしてPermutation importanceを計算してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量のシャッフル\n",
    "test_permuted = test_features.copy()\n",
    "test_permuted[\"TVwatch_day\"] = np.random.permutation(test_permuted[\"TVwatch_day\"])\n",
    "test_permuted.to_csv(\"test_permuted.csv\", header=False, index=False)\n",
    "\n",
    "# 変換後データのアップロード先を指定\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'sagemaker/feature_importance' \n",
    "permuted_channel = prefix + '/test_permuted'\n",
    "\n",
    "\n",
    "# csvファイルとして保存された学習用データ、検証用データをS3バケット上にアップロード\n",
    "sagemaker_session = sagemaker.Session()\n",
    "input_permuted = sagemaker_session.upload_data(path=\"test_permuted.csv\", bucket=bucket, key_prefix=permuted_channel)\n",
    "\n",
    "# アップロードされたデータ\n",
    "s3_input_test = s3_input(s3_data=input_permuted, content_type='text/csv')\n",
    "\n",
    "# アップロード先のパス\n",
    "s3_purmuted_data = 's3://{}/{}'.format(bucket, permuted_channel)\n",
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "\n",
    "# バッチ変換用のインスタンスを設定\n",
    "xgb_transformer = xgb.transformer(instance_count=1,\n",
    "                                  instance_type='ml.m4.xlarge',\n",
    "                                  strategy='MultiRecord',\n",
    "                                  assemble_with='Line',\n",
    "                                  output_path=s3_output_location)\n",
    "\n",
    "# バッチ変換用ジョブを設定\n",
    "xgb_transformer.transform(data=input_permuted, content_type='text/csv', split_type='Line')\n",
    "\n",
    "# バッチ変換の開始\n",
    "xgb_transformer.wait()\n",
    "\n",
    "\n",
    "# 手元でベーススコアを計算するためバッチ変換データのダウンロード\n",
    "s3 = boto3.client('s3')\n",
    "s3.download_file(Bucket=bucket, Key= prefix + \"/output/\" + \"test_permuted.csv.out\", Filename = \"test_permuted.csv.out\")\n",
    "\n",
    "\n",
    "# シャッフルしたデータでの予測値を読み込み\n",
    "test_permuted_predict = pd.read_csv(\"test_permuted.csv.out\", names=[\"predict\"])\n",
    "\n",
    "# 本来の値と予測値の平均二乗誤差を計算\n",
    "permuted_score = mean_squared_error(test_true, test_permuted_predict)\n",
    "permuted_importance = permuted_score - base_score\n",
    "\n",
    "print(\"シャッフル時のスコアは {} です\".format(permuted_score))\n",
    "print(\"Permuted Importanceは {} です\".format(permuted_importance ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### すべての特徴量に対してPermutation importanceを計算\n",
    "他の特徴量についてもPermutation importanceを計算したい場合には下記を実行して下さい。計算に時間がかかるので本ハンズオンの時間の中では実施致しません。\n",
    "実行したい場合には下記のセルを「Raw」から「Code」へ変更し実行して下さい。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def permuted(df):\n",
    "    \"\"\"カラム名と、その特定のカラムをシャッフルしたデータフレームを返す\"\"\"\n",
    "    for column_name in df.columns:\n",
    "        permuted_df = df.copy()\n",
    "        permuted_df[column_name] = np.random.permutation(permuted_df[column_name])\n",
    "        yield column_name, permuted_df\n",
    "        \n",
    "        \n",
    "# 特定のカラムをシャッフルした状態で推論したときのスコアを計算する\n",
    "permuted_test_features_gen = permuted(test_features)\n",
    "permutation_scores = []\n",
    "for column_name, permuted_X_test in permuted_test_features_gen:\n",
    "    print(\"{} のPermuted Importanceをはかっています\".format(column_name))\n",
    "    permuted_X_test.to_csv(\"test_permuted_{}.csv\".format(column_name), header=False, index=False)\n",
    "\n",
    "    # アップロード先を指定\n",
    "    bucket = sess.default_bucket()\n",
    "    prefix = 'sagemaker/feature_importance' \n",
    "    permuted_channel = prefix + '/test_permuted'\n",
    "\n",
    "\n",
    "    # csvファイルとして保存された学習用データ、検証用データをS3バケット上にアップロード\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    input_permuted = sagemaker_session.upload_data(path=\"test_permuted_{}.csv\".format(column_name), bucket=bucket, key_prefix=permuted_channel)\n",
    "\n",
    "    # アップロードされたデータ\n",
    "    s3_input_test = s3_input(s3_data=input_permuted, content_type='text/csv')\n",
    "\n",
    "    # アップロード先のパス\n",
    "    s3_purmuted_data = 's3://{}/{}'.format(bucket, permuted_channel)\n",
    "    s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "\n",
    "    # バッチ変換用のインスタンスを設定\n",
    "    xgb_transformer = xgb.transformer(instance_count=1,\n",
    "                                      instance_type='ml.m4.xlarge',\n",
    "                                      strategy='MultiRecord',\n",
    "                                      assemble_with='Line',\n",
    "                                      output_path=s3_output_location)\n",
    "\n",
    "\n",
    "    # バッチ変換用ジョブを設定\n",
    "    xgb_transformer.transform(data=input_permuted, content_type='text/csv', split_type='Line')\n",
    "    \n",
    "    # バッチ変換の開始\n",
    "    xgb_transformer.wait()\n",
    "    \n",
    "    \n",
    "    # バッチ変換データのダウンロード\n",
    "    s3 = boto3.client('s3')\n",
    "    s3.download_file(Bucket=bucket,\n",
    "                     Key= prefix + \"/output/\" + \"test_permuted_{}.csv.out\".format(column_name),\n",
    "                     Filename = \"test_permuted_{}.csv.out\".format(column_name))\n",
    "    \n",
    "\n",
    "    # シャッフルしたデータでの予測値を読み込み\n",
    "    test_permuted_predict = pd.read_csv(\"test_permuted_{}.csv.out\".format(column_name), names=[\"predict\"])\n",
    "    \n",
    "    # 本来の値と予測値の平均二乗誤差を計算\n",
    "    permuted_score = mean_squared_error(test_true, test_permuted_predict)\n",
    "    permuted_importance = permuted_score - base_score\n",
    "    \n",
    "    print(\"{} のシャッフル時のスコアは {} です\".format(column_name, permuted_score))\n",
    "    print(\"{} のPermuted Importanceは {} です\".format(column_name, permuted_importance ))\n",
    "\n",
    "    permutation_scores.append(permuted_score)\n",
    "    \n",
    "df_permutation_scores = pd.Series(permutation_scores, index=test_features.columns) \n",
    "df_permutation_scores.to_csv(\"permutation_scores.csv\", header=False, index=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('xgboost parmutation importance', fontsize = 20)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.barh(test_features.columns,df_permutation_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実行した結果を下記になります。\n",
    "\n",
    "![title](img/permutation_importances.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Dependency Plotの活用\n",
    "Partial Dependency Plotは特徴量の寄与度だけでなく、その特徴量の変化した際にどの程度目的変数が変化するかを確認するために活用されます。\n",
    "手順は下記になります。\n",
    "- 検証データのある点を元にして、影響を見たい1つの特徴量だけを変化させたデータセットを作成する\n",
    "- そのデータに対して学習済モデルで推論をを実施する\n",
    "- 1つの特徴量が変化するとそれぞれのデータ点の予測はどう変わるのかを計算する\n",
    "- その平均をプロットする\n",
    "\n",
    "今回は素の特徴量で一番\"TVwatch_day\"に対して5個のデータ点で推論することでPartial Dependency Plotを描いていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial(df, target_column):\n",
    "    \"\"\"target featureの値を周辺化したデータフレームを返す\"\"\"\n",
    "    max_value = df[target_column].quantile(0.95)\n",
    "    min_value = df[target_column].quantile(0.05)\n",
    "    interval = (max_value - min_value)/5\n",
    "    \n",
    "    range_target_column = np.arange(min_value,max_value, interval)\n",
    "    for v in range_target_column:\n",
    "        pdp_df = df.copy()\n",
    "        pdp_df[target_column] = v\n",
    "        yield v, pdp_df\n",
    "\n",
    "# 対象となる特徴量を設定      \n",
    "target_column = \"TVwatch_day\"\n",
    "\n",
    "# 対象となる特徴量に対して\n",
    "pdp_test_features_gen = partial(df=test_features, target_column=target_column)\n",
    "\n",
    "df_plot = pd.DataFrame()\n",
    "\n",
    "for pdp_value, pdp_test_features in pdp_test_features_gen:\n",
    "    # 特定のカラムを特定の値にした状態で推論したときのスコアを計算する\n",
    "    print(\"Target feature {} is transformed as {}\".format(target_column, pdp_value))\n",
    "    \n",
    "    pdp_test_features.to_csv(\"test_X_{}_{}.csv\".format(target_column, pdp_value), header=False, index=False)\n",
    "\n",
    "    # アップロード先を指定\n",
    "    bucket = sess.default_bucket()\n",
    "    prefix = 'sagemaker/feature_importance' \n",
    "    pdp_channel = prefix + '/test_pdp'\n",
    "\n",
    "\n",
    "    # csvファイルとして保存された学習用データ、検証用データをS3バケット上にアップロード\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    input_pdp = sagemaker_session.upload_data(path=\"test_X_{}_{}.csv\".format(target_column, pdp_value), bucket=bucket, key_prefix=pdp_channel)\n",
    "\n",
    "    # アップロードされたデータ\n",
    "    s3_input_test = s3_input(s3_data=input_pdp, content_type='text/csv')\n",
    "\n",
    "    # アップロード先のパス\n",
    "    s3_pdp_data = 's3://{}/{}'.format(bucket, pdp_channel)\n",
    "    s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "\n",
    "    # バッチ変換用のインスタンスを設定\n",
    "    xgb_transformer = xgb.transformer(instance_count=4,\n",
    "                                      instance_type='ml.m4.4xlarge',\n",
    "                                      strategy='MultiRecord',\n",
    "                                      assemble_with='Line',\n",
    "                                      output_path=s3_output_location)\n",
    "\n",
    "    # バッチ変換用ジョブの実行\n",
    "    xgb_transformer.transform(data=input_pdp,\n",
    "                              # job_name =  \"transform_{}_{}\".format(target_column, pdp_value),\n",
    "                              content_type='text/csv',\n",
    "                              split_type='Line')\n",
    "    \n",
    "    # バッチ変換待機\n",
    "    xgb_transformer.wait()    \n",
    "    \n",
    "    # バッチ変換データのダウンロード\n",
    "    s3 = boto3.client('s3')\n",
    "    s3.download_file(Bucket=bucket,\n",
    "                     Key= prefix + \"/output/\" + \"test_X_{}_{}.csv.out\".format(target_column, pdp_value),\n",
    "                     Filename = \"test_X_{}_{}.csv.out\".format(target_column, pdp_value))\n",
    "    \n",
    "    df_tmp =  pd.read_csv(\"test_X_{}_{}.csv.out\".format(target_column, pdp_value), names=[str(pdp_value)])\n",
    "    df_plot = pd.concat([df_plot, df_tmp], axis=1)\n",
    "    \n",
    "\n",
    "# プロットの実行\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('xgboost partial dependency plot', fontsize = 20)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.plot(df_plot.mean().index,df_plot.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考\n",
    "XGBoostのアルゴリズムについて\n",
    "- [XGBoost: A Scalable Tree Boosting System(XGBoostの論文)](https://arxiv.org/abs/1603.02754)\n",
    "- [XGBoost論文を丁寧に解説する](https://qiita.com/triwave33/items/aad60f25485a4595b5c8)\n",
    "\n",
    "XGBoostを含む木系アルゴリズムの解釈について\n",
    "- [特徴量重要度にバイアスが生じる状況ご存知ですか？](https://aotamasaki.hatenablog.com/entry/bias_in_feature_importances)\n",
    "- [Python: 特徴量の重要度を Permutation Importance で計測する](https://blog.amedama.jp/entry/permutation-importance)\n",
    "- [Machine Learning Explainability](https://www.kaggle.com/learn/machine-learning-explainability)\n",
    "- [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
