{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## はじめに\n",
    "本ハンズオンではSageMakerの組み込みアルゴリズムを活用した機械学習を体験頂きます。学習に使うデータをAmazon S3のバケットへアップロードし、組み込みアルゴリズムとSageMakerの学習用インスタンスを使ってモデルを学習、学習されたモデルを推論インスタンスへデプロイする流れをご確認下さい。\n",
    "\n",
    "今回はテキストデータについて類似の単語群を探すというタスクを下記の手順で行います。\n",
    "- wikipedia由来のデータ用いてWord2Vecモデルを学習\n",
    "- Word2Vecモデルが適切に学習されているかを確認\n",
    "    - テキストデータをWrod2Vecで100次元のベクトル表現へ変換\n",
    "    - 100次元のベクトルをt-SNEを用いて2次元のベクトルへ次元圧縮\n",
    "    - 2次元データを散布図で可視化し定性的に評価\n",
    "- 100次元データをK-Meansアルゴリズムを用いてクラスタリングし、似た表現のグループを作成\n",
    "- 未知のデータをWord2Vecで100次元ベクトル化した上で、上述のどのグループに所属するかを計算\n",
    "\n",
    "今後のプロトタイピングを実施するに辺り、本ハンズオンをとおしてご確認を頂きたい概念は下記です。\n",
    "- SageMakerの組み込みアルゴリズムを用いた機械学習の実施\n",
    "- テキストデータをベクトルへ変換するとはどういうことか\n",
    "- 次元圧縮するとはどういうことか\n",
    "- クラスタリングとは何か\n",
    "\n",
    "逆に、現時点で時間をかけて頂かなくて良い点は下記になります。\n",
    "- 機械学習アルゴリズムそのもの(今回で言えばWord2Vec、t-SNE、K-Meansなど自体）\n",
    "- Pythonライブラリのそれぞれの文法や詳細(numpy, pandas, scikit-learn、SageMakerなど)\n",
    "- 今回はほとんど行っていない自然言語処理におけるデータの前処理(ノイズの除去や単語の正規化、ストップワードの除去など)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの準備\n",
    "今回はWikipediaから抽出した文章に対してクリーニングなどの処理をした後の[ja.text8](https://github.com/Hironsan/ja.text8)という100MB程度のデータ(コーパス)を活用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データのダウンロードと解凍\n",
    "!wget https://s3-ap-northeast-1.amazonaws.com/dev.tech-sketch.jp/chakki/public/ja.text8.zip\n",
    "!unzip -o ja.text8.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを読み込み、中身を確認します。総単語数、ユニークな単語数、文書を見ます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "\n",
    "# ファイルの読み込み\n",
    "with open(\"ja.text8\") as f:\n",
    "    words = f.read().split()\n",
    "    \n",
    "print(\"総単語数:{}\".format(len(words)))\n",
    "print(\"ユニークな単語数：{}\".format(len(set(words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# どのような文章か先頭300単語を表示\n",
    "\"\".join(words[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データのアップロード\n",
    "SageMakerの学習インスタンスが利用できるよう、データをS3のバケットへアップロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アップロード先を指定\n",
    "prefix = 'sagemaker/DEMO-blazingtext-text8' \n",
    "train_channel = prefix + '/train'\n",
    "\n",
    "# S3へアップロード\n",
    "bucket = sess.default_bucket() # Replace with your own bucket name if needed\n",
    "sess.upload_data(path='ja.text8', bucket=bucket, key_prefix=train_channel)\n",
    "\n",
    "# アップロード先のパス\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n",
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vecアルゴリズムの学習\n",
    "今回はSageMakerのWord2Vecの組み込みアルゴリズムであるBlazingTextを活用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = boto3.Session().region_name\n",
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "\n",
    "\n",
    "# どのような学習を行うかの設定\n",
    "bt_model = sagemaker.estimator.Estimator(container,\n",
    "                                         role, \n",
    "                                         train_instance_count=2, \n",
    "                                         train_instance_type='ml.c4.2xlarge',\n",
    "                                         train_volume_size = 5,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)\n",
    "\n",
    "\n",
    "# Word2Vecアルゴリズムのハイパラ設定\n",
    "bt_model.set_hyperparameters(mode=\"batch_skipgram\",\n",
    "                             epochs=5,\n",
    "                             min_count=5,\n",
    "                             sampling_threshold=0.0001,\n",
    "                             learning_rate=0.05,\n",
    "                             window_size=5,\n",
    "                             vector_dim=100,\n",
    "                             negative_samples=5,\n",
    "                             batch_size=11, #  = (2*window_size + 1) (Preferred. Used only if mode is batch_skipgram)\n",
    "                             evaluation=True,# Perform similarity evaluation on WS-353 dataset at the end of training\n",
    "                             subwords=False) # Subword embedding learning is not supported by batch_skipgram\n",
    "\n",
    "# 学習データの指定\n",
    "train_data = sagemaker.session.s3_input(s3_train_data,\n",
    "                                        distribution='FullyReplicated',\n",
    "                                        content_type='text/plain',\n",
    "                                        s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data}\n",
    "\n",
    "\n",
    "# 学習の開始\n",
    "bt_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vecを使って単語のベクトル表現を得る\n",
    "まずは学習されたWord2Vecを推論エンドポイントへデプロイして、推論(単語のベクトル表現化)ができる状態にします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_endpoint = bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "準備ができたので単語をベクトルに変換してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自転車や女性という単語を推論するようリクエストを作成\n",
    "words = [\"自動車\", \"女性\"]\n",
    "request = json.dumps({\"instances\" : words})\n",
    "\n",
    "# リクエストを推論エンドポイントへ投げ、レスポンスを得る\n",
    "response = bt_endpoint.predict(request)\n",
    "\n",
    "# レスポンスの中身を確認\n",
    "vecs = json.loads(response)\n",
    "print(vecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vecs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vecの評価\n",
    "ja.text8で学習したモデルが適切に単語をベクトル化できているか確認するために、ja.text8の単語群のベクトル表現(100次元ベクトル)を入手し、t-SNEアルゴリズムで2次元のベクトル表現まで次元圧縮を行った上で、散布図を描いて可視化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "key = bt_model.model_data[bt_model.model_data.find(\"/\", 5)+1:]\n",
    "s3.Bucket(bucket).download_file(key, 'model.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ダウンロードしてきた学習済モデルのベクトル表現を解凍します。`vector.txt`がベクトル表現です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ベクトル表現はデータは頻度での降順で保存されているため上位400データを可視化対象とします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# 使用するデータ数を400と指定\n",
    "num_points = 400\n",
    "\n",
    "# 可視化しやすいよう前処理を行います。\n",
    "first_line = True\n",
    "index_to_word = []\n",
    "with open(\"vectors.txt\",\"r\") as f:\n",
    "    for line_num, line in enumerate(f):\n",
    "        if first_line:\n",
    "            dim = int(line.strip().split()[1])\n",
    "            word_vecs = np.zeros((num_points, dim), dtype=float)\n",
    "            first_line = False\n",
    "            continue\n",
    "        line = line.strip()\n",
    "        word = line.split()[0]\n",
    "        vec = word_vecs[line_num-1]\n",
    "        for index, vec_val in enumerate(line.split()[1:]):\n",
    "            vec[index] = float(vec_val)\n",
    "        index_to_word.append(word)\n",
    "        if line_num >= num_points:\n",
    "            break\n",
    "word_vecs = normalize(word_vecs, copy=False, return_norm=False)\n",
    "\n",
    "# t-SNEアルゴリズムのハイパラ設定\n",
    "tsne = TSNE(perplexity=40, n_components=2, init='pca', n_iter=10000)\n",
    "# 100次元ベクトルから2次元ベクトルへ変換\n",
    "two_d_embeddings = tsne.fit_transform(word_vecs[:num_points])\n",
    "labels = index_to_word[:num_points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可視化のための準備として日本語フォントをダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo yum install -y ipa-gothic-fonts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    " \n",
    "mpl.font_manager._rebuild() #キャッシュの削除\n",
    "plt.rcParams['font.family'] = 'IPAGothic' # インストールしたフォントを指定\n",
    "\n",
    "\n",
    "def plot(embeddings, labels):\n",
    "    plt.figure(figsize=(20,20))\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = embeddings[i,:]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',\n",
    "                       ha='right', va='bottom')\n",
    "    plt.savefig('image.png')\n",
    "    plt.show()\n",
    "\n",
    "plot(two_d_embeddings, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 単語のクラスタリング\n",
    "ベクトル化された単語群をK-Meansアルゴリズムを使ってクラスタリングしグループを作ります。K-Meansに関してもSageMakerの組み込みアルゴリズムを使います。\n",
    "Word2Vecの時と同様にS3バケットへデータ(100次元ベクトル表現)をアップロードし、モデルを学習、推論エンドポイントへモデルをデプロイした上でモデル評価を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import KMeans\n",
    "\n",
    "# K-Meansアルゴリズムに学習できる形式へデータタイプを変換\n",
    "word_vecs32 = word_vecs.astype('float32')\n",
    "\n",
    "# アップロード先のパス\n",
    "data_location = 's3://{}/kmeans_highlevel_example/data'.format(bucket)\n",
    "output_location = 's3://{}/kmeans_example/output'.format(bucket)\n",
    "\n",
    "# K-Meansアルゴリズムの設定\n",
    "kmeans = KMeans(role=role,\n",
    "                train_instance_count=2,\n",
    "                train_instance_type='ml.c4.xlarge',\n",
    "                output_path=output_location,\n",
    "                k=10,\n",
    "                data_location=data_location)\n",
    "\n",
    "# 組み込みアルゴリズム用のデータ形式へ変換してS3へアップロード\n",
    "inputs = kmeans.record_set(word_vecs32)\n",
    "\n",
    "# 学習の開始\n",
    "kmeans.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# クラスタリングモデルの評価\n",
    "まずは学習されたK-Meansモデルを推論エンドポイントへデプロイして、推論(ベクトル表現のグループ化)ができる状態にします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_endpoint = kmeans.deploy(initial_instance_count=1,\n",
    "                                instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は400個の単語がを10個のグループにクラスタリングしました。どのような単語がどのグループに所属するか確認しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用に使ったデータ(10次元ベクトル表現データ)を推論に使い、クラスを得る。\n",
    "result = kmeans_endpoint.predict(word_vecs32)\n",
    "\n",
    "clusters = [r.label['closest_cluster'].float32_tensor.values[0] for r in result]\n",
    "for cluster in range(10):\n",
    "    words = [word for l, word in zip(clusters, labels) if int(l) == cluster]\n",
    "    print('クラスタ{}に属する単語は下記となります'.format(cluster))\n",
    "    print(words)\n",
    "    print('============================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 入力する任意の文字列に対するクラスタリング\n",
    "新しい単語に対してはWord2Vecでベクトル化した上で、K-Meansにてどのグループに所属するかを計算します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新しい単語を入力します。\n",
    "input_word = '自動車'\n",
    "new_word = [input_word]\n",
    "\n",
    "# Word2Vecの推論エンドポイントへリクエストとして入力しベクトル表現を得る\n",
    "payload = {\"instances\" : new_word}\n",
    "response = bt_endpoint.predict(json.dumps(payload))\n",
    "new_vec = normalize(np.array(json.loads(response)[0]['vector']).reshape(1, -1), copy=False, return_norm=False).astype('float32')\n",
    "\n",
    "# K-Meansの推論エンドポイントへベクトル表現を入力し所属するグループを得る\n",
    "result = kmeans_endpoint.predict(new_vec)\n",
    "\n",
    "# 同じクラスに所属する単語を表示する\n",
    "similar_words = [word for l, word in zip(clusters, labels) if int(l) == int(result[0].label['closest_cluster'].float32_tensor.values[0])]\n",
    "print('入力された文字列は「{}」です'.format(input_word))\n",
    "print('====類似性の近い単語群は下記です。＝＝＝＝＝＝')\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論エンドポイントの削除\n",
    "エンドポイントは起動したままだとコストがかかります。不要な場合は削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(bt_endpoint.endpoint)\n",
    "sagemaker.Session().delete_endpoint(kmeans_endpoint.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考\n",
    "- [機械学習のための特徴量エンジニアリング](https://www.oreilly.co.jp/books/9784873118680/)\n",
    "    - 3章テキストデータの取り扱い\n",
    "    - 4章特徴量スケーリングによる効果\n",
    "- [自然言語処理における前処理の種類とその威力](https://qiita.com/Hironsan/items/2466fe0f344115aff177)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
